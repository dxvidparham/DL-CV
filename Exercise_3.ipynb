{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.7.3"
    },
    "colab": {
      "name": "Exercise_3.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XPPysUMOZxgz"
      },
      "source": [
        "# Exercise 3\n",
        "## Generative adversarial networks\n",
        "### Generating of MNIST digits with a GAN\n",
        "\n",
        "First we import the modules we need"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TlFFUmfoZxg1"
      },
      "source": [
        "import numpy as np\n",
        "from tqdm.notebook import tqdm\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision.datasets as datasets\n",
        "from torch.utils.data import DataLoader\n",
        "import torchvision.transforms as transforms\n",
        "from IPython import display\n",
        "import matplotlib.pylab as plt\n",
        "import ipywidgets"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UKTYJngTuOtG"
      },
      "source": [
        "Check that we're running on GPU, and initialize our device"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RLcplSTKZxg4"
      },
      "source": [
        "if torch.cuda.is_available():\n",
        "    print(\"The code will run on GPU.\")\n",
        "else:\n",
        "    print(\"The code will run on CPU. Go to Edit->Notebook Settings and choose GPU as the hardware accelerator\")\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lV2YzpnSuUwt"
      },
      "source": [
        "Setup our MNIST dataloaders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hSXlDTz0Zxg7"
      },
      "source": [
        "batch_size = 64\n",
        "trainset = datasets.MNIST('./data', train=True, download=True, transform=transforms.ToTensor())\n",
        "train_loader = DataLoader(trainset, batch_size=batch_size, shuffle=True)\n",
        "testset = datasets.MNIST('./data', train=False, download=True, transform=transforms.ToTensor())\n",
        "test_loader = DataLoader(testset, batch_size=batch_size, shuffle=False)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dhxprWbiZxg-"
      },
      "source": [
        "Implement your **generator** network as a fully connected neural network.\n",
        "\n",
        "You could start with a network that:\n",
        "* takes as input a 100 long vector\n",
        "* has four hidden layers with 2848 neurons\n",
        "* uses LeakyReLU as the activation function\n",
        "* uses BatchNorm\n",
        "* has Tanh as the last layer (we work with MNIST in the -1 to 1 range)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aHYx5QejZxg_"
      },
      "source": [
        "class Generator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Generator, self).__init__()\n",
        "        ....\n",
        "        \n",
        "    def forward(self, x):\n",
        "        ....\n",
        "        x = x.view(x.size(0), 1, 28, 28)\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tGcxpGzQZxhD"
      },
      "source": [
        "Implement your **discriminator** network as a fully connected neural network\n",
        "\n",
        "Start out with a network that\n",
        "* takes as input an $28\\times28$ image\n",
        "* has three hidden layers with [1024, 512, 256] neurons respectively\n",
        "* uses LeakyReLU as the activation function\n",
        "* uses Dropout\n",
        "* has no activation on the final layer (we will call sigmoid if we want a probability)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e7GEvrDvZxhD"
      },
      "source": [
        "class Discriminator(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(Discriminator, self).__init__()\n",
        "        ....\n",
        "    \n",
        "    def forward(self, x):\n",
        "        x = x.view(x.size(0), -1)\n",
        "        ....\n",
        "        return x"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "jWDuWiwP-pj6"
      },
      "source": [
        "Now let's train our model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fn8RKyxLZxhG"
      },
      "source": [
        "#Initialize networks\n",
        "d = Discriminator().to(device)\n",
        "g = Generator().to(device)\n",
        "d_opt = torch.optim.Adam(d.parameters(), 0.0004, (0.5, 0.999))\n",
        "g_opt = torch.optim.Adam(g.parameters(), 0.0001, (0.5, 0.999))\n",
        "\n",
        "plt.figure(figsize=(20,10))\n",
        "subplots = [plt.subplot(2, 6, k+1) for k in range(12)]\n",
        "num_epochs = 10\n",
        "discriminator_final_layer = torch.sigmoid\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "    for minibatch_no, (x, target) in enumerate(train_loader):\n",
        "        x_real = x.to(device)*2-1 #scale to (-1, 1) range\n",
        "        z = torch.randn(x.shape[0], 100).to(device)\n",
        "        x_fake = g(z)\n",
        "        #Update discriminator\n",
        "        d.zero_grad()\n",
        "        #remember to detach x_fake before using it to compute the discriminator loss\n",
        "        #otherwise the discriminator loss will backpropagate through the generator as well, which is unnecessary.\n",
        "        d_loss= ... \n",
        "        d_loss.backward()\n",
        "        d_opt.step()\n",
        "\n",
        "        #Update generator\n",
        "        g.zero_grad()\n",
        "        g_loss = ...\n",
        "        g_loss.backward()\n",
        "        g_opt.step()\n",
        "        \n",
        "        assert(not np.isnan(d_loss.item()))\n",
        "        #Plot results every 100 minibatches\n",
        "        if minibatch_no % 100 == 0:\n",
        "            with torch.no_grad():\n",
        "                P = discriminator_final_layer(d(x_fake))\n",
        "                for k in range(11):\n",
        "                    x_fake_k = x_fake[k].cpu().squeeze()/2+.5\n",
        "                    subplots[k].imshow(x_fake_k, cmap='gray')\n",
        "                    subplots[k].set_title('d(x)=%.2f' % P[k])\n",
        "                    subplots[k].axis('off')\n",
        "                z = torch.randn(batch_size, 100).to(device)\n",
        "                H1 = discriminator_final_layer(d(g(z))).cpu()\n",
        "                H2 = discriminator_final_layer(d(x_real)).cpu()\n",
        "                plot_min = min(H1.min(), H2.min()).item()\n",
        "                plot_max = max(H1.max(), H2.max()).item()\n",
        "                subplots[-1].cla()\n",
        "                subplots[-1].hist(H1.squeeze(), label='fake', range=(plot_min, plot_max), alpha=0.5)\n",
        "                subplots[-1].hist(H2.squeeze(), label='real', range=(plot_min, plot_max), alpha=0.5)\n",
        "                subplots[-1].legend()\n",
        "                subplots[-1].set_xlabel('Probability of being real')\n",
        "                subplots[-1].set_title('Discriminator loss: %.2f' % d_loss.item())\n",
        "                \n",
        "                title = 'Epoch {e} - minibatch {n}/{d}'.format(e=epoch+1, n=minibatch_no, d=len(train_loader))\n",
        "                plt.gcf().suptitle(title, fontsize=20)\n",
        "                display.display(plt.gcf())\n",
        "                display.clear_output(wait=True)\n",
        "                "
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "S51bKUY_ZxhJ"
      },
      "source": [
        "Do you get a model to generate nice images?\n",
        "\n",
        "The plot shows probabilities of real and generated digits being classified as real. Is the discriminator able to distinguish real from fake? If not, try increasing the capacity of the discriminator.\n",
        "Feel free to change the architecture as you see fit.\n",
        "\n",
        "## Additional tasks\n",
        "* Change the architecture to get better results\n",
        "* Implement an LSGAN\n",
        "* Implement a WGAN with SN\n",
        "* Convert your network to a DCGAN\n",
        "* Visualize what happens when you interpolate between to points in the latent space\n",
        "* Generate images from FashionMNIST\n",
        "\n",
        "### Harder tasks:\n",
        "* Add data augmentation to fake and real images\n",
        "* Use the data augmentation to the generated images \n",
        "* Convert your architecture into an AC-GAN"
      ]
    }
  ]
}